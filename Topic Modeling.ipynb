{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# numpy, pandas, matplotlib and regular expressions (data science essentials)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "# import en_core_web_sm\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# styling\n",
    "pd.set_option('display.max_columns',150)\n",
    "plt.style.use('bmh')\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.raiseExceptions = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ceo_tweets_final.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose the tweets after 2016\n",
    "df[\"date\"] = pd.to_datetime(df['date'])\n",
    "df = df[df['date'].dt.year>2016]\n",
    "df[\"date\"]=df[\"date\"].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sahana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to remove stop words and punctuation, get mentions and hashtags from tweets and removing links and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing stop words, punctuation and tokenizing\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + ['rt','amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        result = re.findall(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9_]+)\", tweet) #(@[A-Za-z0-9]+)|\n",
    "        return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        result = re.findall(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))#([A-Za-z]+[A-Za-z0-9_]+)\", tweet) #(@[A-Za-z0-9]+)|\n",
    "        return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_split(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        p = ' '.join(re.sub(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        s = ' '.join(re.sub(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))#([A-Za-z]+[A-Za-z0-9_]+)\", \" \", p).split())\n",
    "        return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", s).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    return lemma.lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to get Bigram and corpus for LDA modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    \"\"\"\n",
    "    Get Bigram Model, Corpus, id2word mapping\n",
    "    \"\"\"\n",
    "    bigram = bigrams(df.tweet_tokens_lem)\n",
    "    bigram = [bigram[tweet] for tweet in df.tweet_tokens_lem]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and clean the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mentions\"] = df[\"tweet\"].apply(lambda tweet: get_mentions(tweet))\n",
    "df[\"tags\"] = df[\"tweet\"].apply(lambda tweet: get_hashtags(tweet))\n",
    "\n",
    "df[\"tweet_clean\"] = df[\"tweet\"].apply(lambda tweet: clean_tweet_split(tweet))\n",
    "\n",
    "df[\"tweet_tokens\"] = df[\"tweet_clean\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
    "df[\"tweet_tokens\"] = df[\"tweet_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x not in stop])\n",
    "\n",
    "df[\"tweet_tokens_lem\"] = df[\"tweet_tokens\"].apply(lambda list_of_words: [lemmatize(x) for x in list_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Less Frequent words\n",
    "df[\"tweet_new\"] = df[\"tweet\"].apply(lambda tweet: remove_links(tweet))\n",
    "df[\"tweet_new\"] = df[\"tweet_new\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
    "df[\"tweet_new\"] = df[\"tweet_new\"].apply(lambda list_of_words: [x for x in list_of_words if x not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_analysis(df, username, num_topics):\n",
    "    df_ceo = df[df['username']== username]\n",
    "    \n",
    "    all_words = df_ceo['tweet_new'].sum()\n",
    "    freq_dist = nltk.FreqDist(all_words)\n",
    "    df_fdist=pd.DataFrame(list(freq_dist.items()), columns=['term', 'freq'])\n",
    "    \n",
    "    df_fdist = df_fdist.sort_values(by = 'freq', ascending = False)\n",
    "    df_fdist = df_fdist[df_fdist['freq'] > 1]\n",
    "    \n",
    "    relevant_words = list(df_fdist['term'])\n",
    "    \n",
    "    df_ceo[\"tweet_new\"] = df_ceo[\"tweet_new\"].apply(lambda list_of_words: [x for x in list_of_words if x in relevant_words])\n",
    "    df_ceo[\"tweet_tokens_lem\"] = df_ceo[\"tweet_new\"].apply(lambda list_of_words: [lemmatize(x) for x in list_of_words])\n",
    "    \n",
    "    train_corpus, train_id2word, bigram_train = get_corpus(df_ceo)\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                               corpus=train_corpus,\n",
    "                               num_topics=num_topics,\n",
    "                               id2word=train_id2word,\n",
    "                               chunksize=100,\n",
    "                               workers=7, # Num. Processing Cores - 1\n",
    "                               passes=50,\n",
    "                               eval_every = 1,\n",
    "                               per_word_topics=True,\n",
    "                               random_state=11)\n",
    "        lda_train.save('lda_train.model')\n",
    "        \n",
    "    coherence_model_lda = CoherenceModel(model=lda_train, texts=bigram_train, dictionary=train_id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(\"The coherence of the LDA model is\",coherence_lda)\n",
    "    \n",
    "    train_vecs = []\n",
    "    for i in range(len(df_ceo.tweet_new)):\n",
    "        top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
    "        topic_vec = [top_topics[i][1] for i in range(num_topics)]\n",
    "        train_vecs.append(topic_vec)\n",
    "    \n",
    "    return df_ceo, lda_train.print_topics(), train_vecs, num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_topics(values):\n",
    "    topics = []\n",
    "    if len(list(set(values))) == 1:\n",
    "        topics = values     \n",
    "    else:\n",
    "        topics.append(max(values))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def assign_topics(col1, col2):\n",
    "    if col1 in col2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tim Cook - Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.3803925329617725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.103*\"year\" + 0.076*\"see\" + 0.072*\"developer\" + 0.058*\"world\" + 0.054*\"time\" + 0.045*\"week\" + 0.038*\"ago\" + 0.036*\"app\" + 0.035*\"powerful\" + 0.034*\"today\"'),\n",
       " (1,\n",
       "  '0.156*\"heart\" + 0.085*\"community\" + 0.079*\"family\" + 0.078*\"one\" + 0.046*\"today\" + 0.046*\"victim\" + 0.046*\"affected\" + 0.043*\"pro\" + 0.042*\"violence\" + 0.041*\"ipad\"'),\n",
       " (2,\n",
       "  '0.180*\"u\" + 0.060*\"make\" + 0.059*\"every\" + 0.053*\"celebrate\" + 0.050*\"life\" + 0.049*\"congratulation\" + 0.044*\"let\" + 0.040*\"day\" + 0.037*\"today\" + 0.036*\"people\"'),\n",
       " (3,\n",
       "  '0.170*\"thank\" + 0.148*\"work\" + 0.116*\"proud\" + 0.090*\"team\" + 0.072*\"great\" + 0.050*\"friend\" + 0.041*\"visit\" + 0.038*\"back\" + 0.037*\"th\" + 0.034*\"help\"'),\n",
       " (4,\n",
       "  '0.103*\"woman\" + 0.080*\"apple\" + 0.075*\"never\" + 0.065*\"story\" + 0.064*\"country\" + 0.062*\"right\" + 0.058*\"men\" + 0.058*\"enjoy\" + 0.057*\"like\" + 0.054*\"place\"'),\n",
       " (5,\n",
       "  '0.208*\"thanks\" + 0.123*\"iphone\" + 0.102*\"new\" + 0.076*\"love\" + 0.058*\"student\" + 0.047*\"forward\" + 0.043*\"thing\" + 0.041*\"shotoniphone\" + 0.033*\"customer\" + 0.031*\"photo\"'),\n",
       " (6,\n",
       "  '0.228*\"apple\" + 0.095*\"today\" + 0.057*\"thrilled\" + 0.055*\"store\" + 0.055*\"new\" + 0.052*\"great\" + 0.051*\"many\" + 0.045*\"future\" + 0.041*\"meet\" + 0.036*\"thanks\"'),\n",
       " (7,\n",
       "  '0.136*\"apple\" + 0.120*\"everyone\" + 0.102*\"around_world\" + 0.074*\"celebrating\" + 0.064*\"happy\" + 0.056*\"day\" + 0.047*\"thank\" + 0.044*\"wishing\" + 0.041*\"customer\" + 0.040*\"first\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@tim_cook', 8)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Technology','Social','People','Gratitude','Women Appreciation','Product','Store Launch','Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tim_cook = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tim_cook['all_topics']= df_tim_cook[['Technology','Social','People','Gratitude','Women Appreciation','Product','Store Launch','Emotion']].values.tolist()\n",
    "df_tim_cook['max_topics'] = df_tim_cook['all_topics'].apply(lambda values: get_max_topics(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tim_cook['Technology'] = df_tim_cook.apply(lambda x: assign_topics(x['Technology'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Social'] = df_tim_cook.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_tim_cook['People'] = df_tim_cook.apply(lambda x: assign_topics(x['People'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Gratitude'] = df_tim_cook.apply(lambda x: assign_topics(x['Gratitude'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Women Appreciation'] = df_tim_cook.apply(lambda x: assign_topics(x['Women Appreciation'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Product'] = df_tim_cook.apply(lambda x: assign_topics(x['Product'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Store Launch'] = df_tim_cook.apply(lambda x: assign_topics(x['Store Launch'], x['max_topics']), axis=1)\n",
    "df_tim_cook['Emotion'] = df_tim_cook.apply(lambda x: assign_topics(x['Emotion'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Technology            117\n",
       "Social                 95\n",
       "People                113\n",
       "Gratitude             102\n",
       "Women Appreciation     86\n",
       "Product                93\n",
       "Store Launch          104\n",
       "Emotion               124\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_topic_weights = df_tim_cook[['Technology','Social','People','Gratitude','Women Appreciation','Product','Store Launch','Emotion']].sum(axis=0)\n",
    "average_topic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Social</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>People</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gratitude</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women Appreciation</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Product</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Store Launch</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Emotion</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index    0\n",
       "0          Technology  117\n",
       "1              Social   95\n",
       "2              People  113\n",
       "3           Gratitude  102\n",
       "4  Women Appreciation   86\n",
       "5             Product   93\n",
       "6        Store Launch  104\n",
       "7             Emotion  124"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo_topics = pd.DataFrame(average_topic_weights)\n",
    "ceo_topics = ceo_topics.reset_index()\n",
    "ceo_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"Products and Services\" if any(y in x.lower() for y in [\"product\",\"store launch\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408e087b00>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chart_studio.plotly as py\n",
    "py.plotly.tools.set_credentials_file(username='sah_lumos', api_key='9fCFTwIksEv3WNQFIZSL')\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Tim Cook')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = df_tim_cook['tags'].apply(pd.Series).stack()\n",
    "\n",
    "hashtags_df= pd.DataFrame(hashtags)\n",
    "hashtags_df.columns=['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df = pd.DataFrame(hashtags_df['hashtags'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df['index'] = hashtags_df['index'].apply(lambda x:\"Holiday Celebration\" if any(y in x.lower() for y in [\"easter\",\"day\",\"diwali\",\"july\",\"month\",\"year\",\"thanksgiving\",\"week\"]) else x)\n",
    "hashtags_df['index'] = hashtags_df['index'].apply(lambda x:\"Apple\" if any(y in x.lower() for y in [\"apple\",\"airpod\",\"iphone\",\"ipad\",\"potrait\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df = pd.DataFrame(hashtags_df[\"index\"].value_counts()).reset_index()\n",
    "hashtags_df.columns= [\"hashtags\",\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.613861386138616"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hashtags_df[hashtags_df[\"hashtags\"] == 'Holiday Celebration'][\"count\"].sum()/hashtags_df[\"count\"].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.881188118811881"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hashtags_df[hashtags_df[\"hashtags\"] == 'Apple'][\"count\"].sum()/hashtags_df[\"count\"].sum())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bill Gates - Bill Gates and Melinda Gates Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.38945760812188884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.157*\"one\" + 0.077*\"book\" + 0.054*\"year\" + 0.053*\"lot\" + 0.029*\"recently\" + 0.029*\"read\" + 0.027*\"favorite\" + 0.025*\"people\" + 0.023*\"great\" + 0.023*\"never\"'),\n",
       " (1,\n",
       "  '0.058*\"need\" + 0.047*\"vaccine\" + 0.035*\"alzheimer\" + 0.033*\"excited\" + 0.033*\"ever\" + 0.033*\"world\" + 0.031*\"new\" + 0.030*\"disease\" + 0.030*\"government\" + 0.028*\"look\"'),\n",
       " (2,\n",
       "  '0.057*\"melinda\" + 0.048*\"life\" + 0.047*\"work\" + 0.047*\"great\" + 0.034*\"new\" + 0.034*\"best\" + 0.029*\"day\" + 0.029*\"student\" + 0.027*\"learn\" + 0.026*\"every\"'),\n",
       " (3,\n",
       "  '0.050*\"like\" + 0.049*\"time\" + 0.041*\"warrenbuffett\" + 0.040*\"melindagates\" + 0.033*\"future\" + 0.030*\"know\" + 0.028*\"new\" + 0.028*\"think\" + 0.028*\"india\" + 0.027*\"give\"'),\n",
       " (4,\n",
       "  '0.103*\"world\" + 0.072*\"progress\" + 0.047*\"see\" + 0.046*\"health\" + 0.035*\"making\" + 0.033*\"child\" + 0.033*\"incredible\" + 0.032*\"global\" + 0.030*\"u\" + 0.026*\"life\"'),\n",
       " (5,\n",
       "  '0.097*\"people\" + 0.057*\"help\" + 0.045*\"today\" + 0.039*\"world\" + 0.038*\"energy\" + 0.031*\"thing\" + 0.030*\"make\" + 0.030*\"get\" + 0.025*\"young\" + 0.023*\"impact\"')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@BillGates', 6)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Book Recommendations','Diseases/Vaccines','Education','Warren Buffet','General World Issues','Renewable Energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bill_gates = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_bill_gates['all_topics']= df_bill_gates[['Book Recommendations','Diseases/Vaccines','Education','Warren Buffet','General World Issues','Renewable Energy']].values.tolist()\n",
    "df_bill_gates['max_topics'] = df_bill_gates['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_bill_gates['Book Recommendations'] = df_bill_gates.apply(lambda x: assign_topics(x['Book Recommendations'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Diseases/Vaccines'] = df_bill_gates.apply(lambda x: assign_topics(x['Diseases/Vaccines'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Education'] = df_bill_gates.apply(lambda x: assign_topics(x['Education'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Warren Buffet'] = df_bill_gates.apply(lambda x: assign_topics(x['Warren Buffet'], x['max_topics']), axis=1)\n",
    "df_bill_gates['General World Issues'] = df_bill_gates.apply(lambda x: assign_topics(x['General World Issues'], x['max_topics']), axis=1)\n",
    "df_bill_gates['Renewable Energy'] = df_bill_gates.apply(lambda x: assign_topics(x['Renewable Energy'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_bill_gates[['Book Recommendations','Diseases/Vaccines','Education','Warren Buffet','General World Issues','Renewable Energy']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d998160>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Bill Gates')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elon Musk - Tesla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.49431548649757723\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@elonmusk', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.229*\"erdayastronaut\" + 0.064*\"martinengwicht\" + 0.054*\"spacex\" + 0.054*\"john_gardi\" + 0.050*\"engine\" + 0.037*\"13ericralph31\" + 0.033*\"raptor\" + 0.025*\"janeidyeve\" + 0.024*\"djsnm\" + 0.020*\"spexcast\"'),\n",
       " (1,\n",
       "  '0.115*\"tesla\" + 0.091*\"great\" + 0.068*\"sure\" + 0.061*\"work\" + 0.053*\"mode\" + 0.044*\"robotbeat\" + 0.042*\"team\" + 0.038*\"need\" + 0.029*\"safety\" + 0.024*\"engineering\"'),\n",
       " (2,\n",
       "  '0.185*\"spacex\" + 0.082*\"launch\" + 0.066*\"starship\" + 0.059*\"first\" + 0.044*\"test\" + 0.036*\"flight\" + 0.028*\"orbit\" + 0.025*\"falcon_9\" + 0.021*\"texas\" + 0.020*\"fire\"'),\n",
       " (3,\n",
       "  '0.111*\"like\" + 0.078*\"true\" + 0.062*\"tunnel\" + 0.062*\"would\" + 0.058*\"teslaownerssv\" + 0.049*\"look\" + 0.043*\"speed\" + 0.035*\"well\" + 0.035*\"steel\" + 0.031*\"people\"'),\n",
       " (4,\n",
       "  '0.188*\"flcnhvy\" + 0.108*\"good\" + 0.068*\"erdayastronaut\" + 0.051*\"make\" + 0.049*\"worldandscience\" + 0.035*\"use\" + 0.035*\"000\" + 0.031*\"keego73\" + 0.025*\"since\" + 0.025*\"harrystoltz1\"'),\n",
       " (5,\n",
       "  '0.222*\"tesla\" + 0.064*\"cleantechnica\" + 0.043*\"model\" + 0.038*\"car\" + 0.034*\"u\" + 0.034*\"also\" + 0.029*\"take\" + 0.027*\"order\" + 0.021*\"x\" + 0.020*\"come\"'),\n",
       " (6,\n",
       "  '0.134*\"tesla\" + 0.052*\"thanks\" + 0.050*\"fredericlambert\" + 0.048*\"autopilot\" + 0.046*\"elonmusk\" + 0.042*\"ok\" + 0.038*\"better\" + 0.036*\"even\" + 0.031*\"amazing\" + 0.028*\"going\"'),\n",
       " (7,\n",
       "  '0.108*\"nasaspaceflight\" + 0.102*\"jeromejaccard_robotbeat\" + 0.102*\"alan1bernard\" + 0.068*\"nasa\" + 0.064*\"2\" + 0.057*\"haha\" + 0.054*\"annerajb\" + 0.053*\"3\" + 0.037*\"month\" + 0.034*\"28delayslater\"'),\n",
       " (8,\n",
       "  '0.120*\"space_station\" + 0.106*\"spacex\" + 0.054*\"dragon\" + 0.044*\"today\" + 0.044*\"tesla_truth\" + 0.043*\"think\" + 0.037*\"mission\" + 0.030*\"want\" + 0.029*\"road\" + 0.028*\"rocket\"'),\n",
       " (9,\n",
       "  '0.083*\"time\" + 0.080*\"tesla\" + 0.071*\"high\" + 0.051*\"vehicle\" + 0.046*\"solar\" + 0.040*\"back\" + 0.040*\"system\" + 0.038*\"wheel\" + 0.035*\"probably\" + 0.035*\"lot\"'),\n",
       " (10,\n",
       "  '0.196*\"yes\" + 0.133*\"car\" + 0.083*\"tesla\" + 0.077*\"boringcompany\" + 0.059*\"way\" + 0.045*\"new\" + 0.032*\"best\" + 0.028*\"around\" + 0.024*\"must\" + 0.023*\"every\"'),\n",
       " (11,\n",
       "  '0.111*\"exactly\" + 0.074*\"actually\" + 0.066*\"yeah\" + 0.060*\"right\" + 0.060*\"teslarati\" + 0.060*\"many\" + 0.055*\"justpaulinelol\" + 0.046*\"always\" + 0.041*\"space\" + 0.036*\"may\"'),\n",
       " (12,\n",
       "  '0.114*\"model_3\" + 0.104*\"tesla\" + 0.087*\"one\" + 0.034*\"almost\" + 0.033*\"kristennetten\" + 0.033*\"long\" + 0.030*\"soon\" + 0.027*\"drive\" + 0.025*\"coming_soon\" + 0.024*\"insideevs\"'),\n",
       " (13,\n",
       "  '0.063*\"nichegamer\" + 0.054*\"much\" + 0.047*\"10\" + 0.043*\"mar\" + 0.039*\"earth\" + 0.038*\"5\" + 0.036*\"cost\" + 0.036*\"day\" + 0.034*\"know\" + 0.033*\"would\"'),\n",
       " (14,\n",
       "  '0.110*\"year\" + 0.075*\"tesla\" + 0.072*\"next\" + 0.055*\"week\" + 0.047*\"end\" + 0.037*\"production\" + 0.035*\"price\" + 0.028*\"driving\" + 0.027*\"last\" + 0.026*\"point\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['spacex1','Tesla','spacex2','Boring Company','tesla1','Clean Energy Initiatives','tesla2','Social','spacex3','tesla3','tesla4','tesla5','tesla6','clean','Upcoming Events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon_musk = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon_musk['all_topics']= df_elon_musk[['spacex1','Tesla','spacex2','Boring Company','tesla1','Clean Energy Initiatives','tesla2','Social','spacex3','tesla3','tesla4','tesla5','tesla6','clean','Upcoming Events']].values.tolist()\n",
    "df_elon_musk['max_topics'] = df_elon_musk['all_topics'].apply(lambda values: get_max_topics(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon_musk['spacex1'] = df_elon_musk.apply(lambda x: assign_topics(x['spacex1'], x['max_topics']), axis=1)\n",
    "df_elon_musk['Tesla'] = df_elon_musk.apply(lambda x: assign_topics(x['Tesla'], x['max_topics']), axis=1)\n",
    "df_elon_musk['spacex2'] = df_elon_musk.apply(lambda x: assign_topics(x['spacex2'], x['max_topics']), axis=1)\n",
    "df_elon_musk['Boring Company'] = df_elon_musk.apply(lambda x: assign_topics(x['Boring Company'], x['max_topics']), axis=1)\n",
    "df_elon_musk['tesla1'] = df_elon_musk.apply(lambda x: assign_topics(x['tesla1'], x['max_topics']), axis=1)\n",
    "df_elon_musk['Clean Energy Initiatives'] = df_elon_musk.apply(lambda x: assign_topics(x['Clean Energy Initiatives'], x['max_topics']), axis=1)\n",
    "df_elon_musk['tesla2'] = df_elon_musk.apply(lambda x: assign_topics(x['tesla2'], x['max_topics']), axis=1)\n",
    "df_elon_musk['Social'] = df_elon_musk.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_elon_musk['spacex3'] = df_elon_musk.apply(lambda x: assign_topics(x['spacex3'], x['max_topics']), axis=1)\n",
    "df_elon_musk['tesla3'] = df_elon_musk.apply(lambda x: assign_topics(x['tesla3'], x['max_topics']), axis=1)\n",
    "df_elon_musk['tesla4'] = df_elon_musk.apply(lambda x: assign_topics(x['tesla4'], x['max_topics']), axis=1)\n",
    "df_elon_musk['tesla5'] = df_elon_musk.apply(lambda x: assign_topics(x['tesla5'], x['max_topics']), axis=1)\n",
    "df_elon_musk['tesla6'] = df_elon_musk.apply(lambda x: assign_topics(x['tesla6'], x['max_topics']), axis=1)\n",
    "df_elon_musk['clean'] = df_elon_musk.apply(lambda x: assign_topics(x['clean'], x['max_topics']), axis=1)\n",
    "df_elon_musk['Upcoming Events'] = df_elon_musk.apply(lambda x: assign_topics(x['Upcoming Events'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_elon_musk[['spacex1','Tesla','spacex2','Boring Company','tesla1','Clean Energy Initiatives','tesla2','Social','spacex3','tesla3','tesla4','tesla5','tesla6','clean','Upcoming Events']].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics = pd.DataFrame(average_topic_weights)\n",
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"Tesla\" if any(y in x.lower() for y in [\"tesla1\",\"tesla2\",\"tesla3\",\"tesla4\",\"tesla5\",\"tesla6\"]) else x)\n",
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"SpaceX\" if any(y in x.lower() for y in [\"spacex1\",\"spacex2\",\"spacex3\"]) else x)\n",
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"SpaceX\" if any(y in x.lower() for y in [\"clean\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d95a8d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Elon Musk')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Richard Branson - Virgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.3823749189697216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.057*\"first\" + 0.046*\"great\" + 0.033*\"time\" + 0.029*\"virginatlantic\" + 0.023*\"wonderful\" + 0.019*\"air\" + 0.018*\"hollybranson\" + 0.018*\"day\" + 0.017*\"back\" + 0.015*\"ever\"'),\n",
       " (1,\n",
       "  '0.059*\"virginvoyages\" + 0.034*\"year\" + 0.028*\"one\" + 0.022*\"travel\" + 0.020*\"new\" + 0.019*\"take\" + 0.019*\"spaceshiptwo\" + 0.015*\"world\" + 0.014*\"place\" + 0.013*\"million\"'),\n",
       " (2,\n",
       "  '0.032*\"love\" + 0.029*\"new\" + 0.024*\"need\" + 0.023*\"always\" + 0.020*\"show\" + 0.018*\"people\" + 0.017*\"would\" + 0.017*\"business\" + 0.017*\"challenge\" + 0.016*\"thought\"'),\n",
       " (3,\n",
       "  '0.033*\"business\" + 0.031*\"people\" + 0.018*\"one\" + 0.017*\"idea\" + 0.017*\"oceanunite\" + 0.016*\"help\" + 0.016*\"world\" + 0.016*\"great\" + 0.015*\"entrepreneur\" + 0.015*\"way\"'),\n",
       " (4,\n",
       "  '0.050*\"thanks\" + 0.037*\"virgin\" + 0.034*\"work\" + 0.028*\"really\" + 0.021*\"many\" + 0.018*\"u\" + 0.017*\"wonderful\" + 0.017*\"hope\" + 0.016*\"best\" + 0.016*\"people\"'),\n",
       " (5,\n",
       "  '0.065*\"team\" + 0.047*\"virginfamily\" + 0.035*\"see\" + 0.034*\"virgingalactic\" + 0.032*\"much\" + 0.022*\"virgin\" + 0.022*\"virgintrains\" + 0.021*\"new\" + 0.019*\"wonderful\" + 0.016*\"welcome\"'),\n",
       " (6,\n",
       "  '0.067*\"virgingalactic\" + 0.042*\"space\" + 0.025*\"quote\" + 0.024*\"think\" + 0.022*\"week\" + 0.022*\"virgin_orbit\" + 0.021*\"neckercup\" + 0.020*\"time\" + 0.020*\"next\" + 0.019*\"congratulation\"')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@richardbranson', 7)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Gratitude','Virgin Voyages','Employee Appreciation','Social','gratitude2','Team','Virgin Galactic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_richard_bran = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_richard_bran['all_topics']= df_richard_bran[['Gratitude','Virgin Voyages','Employee Appreciation','Social','gratitude2','Team','Virgin Galactic']].values.tolist()\n",
    "df_richard_bran['max_topics'] = df_richard_bran['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_richard_bran['Gratitude'] = df_richard_bran.apply(lambda x: assign_topics(x['Gratitude'], x['max_topics']), axis=1)\n",
    "df_richard_bran['Virgin Voyages'] = df_richard_bran.apply(lambda x: assign_topics(x['Virgin Voyages'], x['max_topics']), axis=1)\n",
    "df_richard_bran['Employee Appreciation'] = df_richard_bran.apply(lambda x: assign_topics(x['Employee Appreciation'], x['max_topics']), axis=1)\n",
    "df_richard_bran['Social'] = df_richard_bran.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_richard_bran['gratitude2'] = df_richard_bran.apply(lambda x: assign_topics(x['gratitude2'], x['max_topics']), axis=1)\n",
    "df_richard_bran['Team'] = df_richard_bran.apply(lambda x: assign_topics(x['Team'], x['max_topics']), axis=1)\n",
    "df_richard_bran['Virgin Galactic'] = df_richard_bran.apply(lambda x: assign_topics(x['Virgin Galactic'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_richard_bran[['Gratitude','Virgin Voyages','Employee Appreciation','Social','gratitude2','Team','Virgin Galactic']].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics = pd.DataFrame(average_topic_weights)\n",
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"Gratitude\" if any(y in x.lower() for y in [\"gratitude\",\"gratitude2\"]) else x)\n",
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"Team\" if any(y in x.lower() for y in [\"team\",\"employee appreciation\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x240843fd390>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Richard Branson')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brian Chesky - AirBnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.4528958594505259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.263*\"airbnb\" + 0.076*\"new\" + 0.058*\"today\" + 0.043*\"idea\" + 0.042*\"first\" + 0.037*\"also\" + 0.032*\"nathanblec\" + 0.032*\"yes\" + 0.031*\"thing\" + 0.030*\"excited\"'),\n",
       " (1,\n",
       "  '0.155*\"home\" + 0.097*\"airbnb\" + 0.077*\"one\" + 0.076*\"thank\" + 0.071*\"people\" + 0.068*\"need\" + 0.047*\"housing\" + 0.046*\"year\" + 0.044*\"last\" + 0.040*\"benedictevans\"'),\n",
       " (2,\n",
       "  '0.149*\"host\" + 0.142*\"airbnb\" + 0.080*\"experience\" + 0.069*\"000\" + 0.059*\"guest\" + 0.052*\"u\" + 0.041*\"city\" + 0.039*\"many\" + 0.036*\"1\" + 0.035*\"year\"'),\n",
       " (3,\n",
       "  '0.124*\"thanks\" + 0.080*\"airbnb\" + 0.075*\"want\" + 0.069*\"like\" + 0.062*\"would\" + 0.059*\"team\" + 0.047*\"2\" + 0.047*\"time\" + 0.042*\"community\" + 0.041*\"part\"')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@bchesky', 4)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Products','Services','Social','Appreciation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bchesky = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_bchesky['all_topics']= df_bchesky[['Products','Services','Social','Appreciation']].values.tolist()\n",
    "df_bchesky['max_topics'] = df_bchesky['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_bchesky['Products'] = df_bchesky.apply(lambda x: assign_topics(x['Products'], x['max_topics']), axis=1)\n",
    "df_bchesky['Services'] = df_bchesky.apply(lambda x: assign_topics(x['Services'], x['max_topics']), axis=1)\n",
    "df_bchesky['Social'] = df_bchesky.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_bchesky['Appreciation'] = df_bchesky.apply(lambda x: assign_topics(x['Appreciation'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_bchesky[['Products','Services','Social','Appreciation']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtags and Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = df_bchesky['tags'].apply(pd.Series).stack()\n",
    "\n",
    "hashtags_df= pd.DataFrame(hashtags)\n",
    "hashtags_df.columns=['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weaccept</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airbnb</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HurricaneIrma</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeAccept</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DACA</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               hashtags\n",
       "weaccept              5\n",
       "Airbnb                4\n",
       "HurricaneIrma         4\n",
       "WeAccept              3\n",
       "DACA                  3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hashtags_df['hashtags'].value_counts()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = df_bchesky['mentions'].apply(pd.Series).stack()\n",
    "\n",
    "mentions_df= pd.DataFrame(mentions)\n",
    "mentions_df.columns=['mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_df = pd.DataFrame(mentions_df['mentions'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_df['index'] = mentions_df['index'].apply(lambda x: \"Airbnb\" if any(y in x.lower() for y in [\"Airbnb\",\"airbnb\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_df.columns= [\"mentions\",\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mentions</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Airbnb</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jgebbia</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bchesky</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nathanblec</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BenedictEvans</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mentions  count\n",
       "0         Airbnb     81\n",
       "1        jgebbia     18\n",
       "2        bchesky     11\n",
       "3     nathanblec     10\n",
       "4  BenedictEvans     10"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.350223546944857"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mentions_df[mentions_df[\"mentions\"] == 'Airbnb'][\"count\"].sum()/mentions_df[\"count\"].sum())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marc Benioff - SalesForce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.3379418862473504\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@Benioff', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.083*\"benioff\" + 0.054*\"salesforce\" + 0.047*\"trailblazer\" + 0.025*\"trailblazerbook\" + 0.020*\"book\" + 0.019*\"1\" + 0.019*\"value\" + 0.018*\"business\" + 0.017*\"ucsf\" + 0.016*\"dreamforce\"'),\n",
       " (1,\n",
       "  '0.134*\"time\" + 0.024*\"time_100\" + 0.023*\"time100\" + 0.023*\"cover\" + 0.023*\"summit\" + 0.022*\"one\" + 0.021*\"live\" + 0.020*\"new\" + 0.017*\"people\" + 0.013*\"world\"'),\n",
       " (2,\n",
       "  '0.070*\"salesforce\" + 0.031*\"trailhead\" + 0.029*\"homelessness\" + 0.020*\"df19\" + 0.015*\"future\" + 0.015*\"know\" + 0.014*\"proud\" + 0.014*\"mkushel\" + 0.014*\"part\" + 0.013*\"dreamforce\"'),\n",
       " (3,\n",
       "  '0.058*\"benioff\" + 0.031*\"amazing\" + 0.030*\"san_francisco\" + 0.021*\"thank\" + 0.017*\"yoshikiofficial\" + 0.017*\"great\" + 0.016*\"friend\" + 0.015*\"people\" + 0.014*\"salesforce_tower\" + 0.014*\"always\"'),\n",
       " (4,\n",
       "  '0.042*\"salesforce\" + 0.031*\"new\" + 0.023*\"need\" + 0.021*\"benioff\" + 0.017*\"million\" + 0.016*\"school\" + 0.015*\"go\" + 0.013*\"facebook\" + 0.010*\"today\" + 0.010*\"2\"'),\n",
       " (5,\n",
       "  '0.032*\"ocean\" + 0.030*\"u\" + 0.026*\"world\" + 0.024*\"make\" + 0.022*\"year\" + 0.017*\"take\" + 0.017*\"today\" + 0.016*\"help\" + 0.016*\"love\" + 0.015*\"company\"')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Author','Time','Company Roadmap','Social','Education','Ocean Conservation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benioff = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_benioff['all_topics']= df_benioff[['Author','Time','Company Roadmap','Social','Education','Ocean Conservation']].values.tolist()\n",
    "df_benioff['max_topics'] = df_benioff['all_topics'].apply(lambda values: get_max_topics(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benioff['Author'] = df_benioff.apply(lambda x: assign_topics(x['Author'], x['max_topics']), axis=1)\n",
    "df_benioff['Time'] = df_benioff.apply(lambda x: assign_topics(x['Time'], x['max_topics']), axis=1)\n",
    "df_benioff['Company Roadmap'] = df_benioff.apply(lambda x: assign_topics(x['Company Roadmap'], x['max_topics']), axis=1)\n",
    "df_benioff['Social'] = df_benioff.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_benioff['Education'] = df_benioff.apply(lambda x: assign_topics(x['Education'], x['max_topics']), axis=1)\n",
    "df_benioff['Ocean Conservation'] = df_benioff.apply(lambda x: assign_topics(x['Ocean Conservation'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_benioff[['Author','Time','Company Roadmap','Social','Education','Ocean Conservation']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x24083ffab38>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Benioff')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bill Gross - IdeaLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.37085428057221376\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@Bill_Gross', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.074*\"yes\" + 0.059*\"thanks\" + 0.053*\"one\" + 0.041*\"smile\" + 0.029*\"great\" + 0.023*\"think\" + 0.022*\"even\" + 0.021*\"amazing\" + 0.020*\"wow\" + 0.019*\"pretty\"'),\n",
       " (1,\n",
       "  '0.061*\"world\" + 0.052*\"amazon\" + 0.051*\"look\" + 0.044*\"amazing\" + 0.041*\"like\" + 0.038*\"would\" + 0.033*\"new\" + 0.028*\"china\" + 0.026*\"love\" + 0.024*\"company\"'),\n",
       " (2,\n",
       "  '0.057*\"people\" + 0.047*\"thing\" + 0.039*\"ted2017\" + 0.039*\"talk\" + 0.038*\"great\" + 0.035*\"company\" + 0.032*\"see\" + 0.029*\"think\" + 0.027*\"way\" + 0.022*\"power\"'),\n",
       " (3,\n",
       "  '0.033*\"1\" + 0.032*\"time\" + 0.031*\"new\" + 0.031*\"year\" + 0.030*\"energy\" + 0.023*\"technology\" + 0.021*\"word\" + 0.021*\"cost\" + 0.019*\"person\" + 0.018*\"day\"'),\n",
       " (4,\n",
       "  '0.064*\"year\" + 0.060*\"davos\" + 0.035*\"wef17\" + 0.029*\"trust\" + 0.028*\"first\" + 0.028*\"10\" + 0.025*\"3\" + 0.021*\"really\" + 0.020*\"last\" + 0.020*\"change\"'),\n",
       " (5,\n",
       "  '0.063*\"ai\" + 0.035*\"human\" + 0.031*\"car\" + 0.029*\"picture\" + 0.024*\"make\" + 0.023*\"ted2017\" + 0.023*\"say\" + 0.022*\"robot\" + 0.021*\"u\" + 0.020*\"idea\"')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Gratitude', 'Appreciation', 'Company', 'Transportation',\n",
    "                      'Renewable Energy', 'Artificial Intelligence' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bill_gross = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_bill_gross['all_topics']= df_bill_gross[['Gratitude', 'Appreciation', 'Company', 'Transportation',\n",
    "                      'Renewable Energy', 'Artificial Intelligence' ]].values.tolist()\n",
    "df_bill_gross['max_topics'] = df_bill_gross['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_bill_gross['Gratitude'] = df_bill_gross.apply(lambda x: assign_topics(x['Gratitude'], x['max_topics']), axis=1)\n",
    "df_bill_gross['Appreciation'] = df_bill_gross.apply(lambda x: assign_topics(x['Appreciation'], x['max_topics']), axis=1)\n",
    "df_bill_gross['Company'] = df_bill_gross.apply(lambda x: assign_topics(x['Company'], x['max_topics']), axis=1)\n",
    "df_bill_gross['Transportation'] = df_bill_gross.apply(lambda x: assign_topics(x['Transportation'], x['max_topics']), axis=1)\n",
    "df_bill_gross['Renewable Energy'] = df_bill_gross.apply(lambda x: assign_topics(x['Renewable Energy'], x['max_topics']), axis=1)\n",
    "df_bill_gross['Artificial Intelligence'] = df_bill_gross.apply(lambda x: assign_topics(x['Artificial Intelligence'], x['max_topics']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_bill_gross[['Gratitude', 'Appreciation', 'Company', 'Transportation',\n",
    "                      'Renewable Energy', 'Artificial Intelligence']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x24083fd5c88>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Bill Gross')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dennis Muilenburg - Boeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.40265589348117425\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@BoeingCEO', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.118*\"great\" + 0.084*\"work\" + 0.079*\"future\" + 0.072*\"today\" + 0.068*\"team\" + 0.042*\"space\" + 0.037*\"air\" + 0.036*\"safety\" + 0.032*\"see\" + 0.029*\"enjoyed\"'),\n",
       " (1,\n",
       "  '0.099*\"first\" + 0.072*\"team\" + 0.048*\"looking\" + 0.047*\"flight\" + 0.047*\"737_max\" + 0.041*\"congratulation\" + 0.039*\"honor\" + 0.038*\"forward\" + 0.036*\"test\" + 0.036*\"week\"'),\n",
       " (2,\n",
       "  '0.141*\"thanks\" + 0.089*\"team\" + 0.075*\"new\" + 0.058*\"year\" + 0.042*\"customer\" + 0.040*\"day\" + 0.039*\"well\" + 0.036*\"member\" + 0.032*\"innovation\" + 0.029*\"many\"'),\n",
       " (3,\n",
       "  '0.083*\"proud\" + 0.069*\"support\" + 0.060*\"service\" + 0.057*\"team\" + 0.044*\"work\" + 0.042*\"boeingdefense\" + 0.039*\"day\" + 0.038*\"life\" + 0.038*\"best\" + 0.036*\"one\"')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Company roadmap','Award','Team/Innovation','Product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boeing = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_boeing['all_topics']= df_boeing[['Company roadmap','Award','Team/Innovation','Product']].values.tolist()\n",
    "df_boeing['max_topics'] = df_boeing['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_boeing['Company roadmap'] = df_boeing.apply(lambda x: assign_topics(x['Company roadmap'], x['max_topics']), axis=1)\n",
    "df_boeing['Award'] = df_boeing.apply(lambda x: assign_topics(x['Award'], x['max_topics']), axis=1)\n",
    "df_boeing['Team/Innovation'] = df_boeing.apply(lambda x: assign_topics(x['Team/Innovation'], x['max_topics']), axis=1)\n",
    "df_boeing['Product'] = df_boeing.apply(lambda x: assign_topics(x['Product'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_boeing[['Company roadmap','Award','Team/Innovation','Product']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x240839a9a90>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Dennis Muilenburg')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### John Legere - TMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.4430009671319855\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@JohnLegere', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.518*\"tmobile\" + 0.057*\"welcome\" + 0.037*\"great\" + 0.037*\"look\" + 0.026*\"nice\" + 0.026*\"tmobilehelp\" + 0.026*\"woo_hoo\" + 0.022*\"sievertmike\" + 0.019*\"good\" + 0.019*\"thanks\"'),\n",
       " (1,\n",
       "  '0.062*\"get\" + 0.048*\"slowcookersunday\" + 0.033*\"day\" + 0.029*\"live\" + 0.029*\"yes\" + 0.028*\"let\" + 0.026*\"go\" + 0.023*\"congrats\" + 0.022*\"happy\" + 0.022*\"time\"'),\n",
       " (2,\n",
       "  '0.073*\"enjoy\" + 0.039*\"jonfreier\" + 0.036*\"better\" + 0.033*\"think\" + 0.033*\"would\" + 0.028*\"always\" + 0.026*\"summer\" + 0.026*\"could\" + 0.026*\"make\" + 0.026*\"need\"'),\n",
       " (3,\n",
       "  '0.043*\"year\" + 0.039*\"love\" + 0.030*\"good\" + 0.029*\"today\" + 0.028*\"team\" + 0.026*\"nevilleray\" + 0.025*\"help\" + 0.023*\"back\" + 0.022*\"well\" + 0.022*\"happy_birthday\"'),\n",
       " (4,\n",
       "  '0.068*\"magenta\" + 0.068*\"one\" + 0.050*\"best\" + 0.050*\"new\" + 0.032*\"favorite\" + 0.030*\"way\" + 0.028*\"know\" + 0.025*\"amazing\" + 0.024*\"love\" + 0.024*\"really\"'),\n",
       " (5,\n",
       "  '0.133*\"tmobile\" + 0.105*\"verizon\" + 0.080*\"att\" + 0.055*\"customer\" + 0.051*\"thank\" + 0.044*\"mobile\" + 0.034*\"com\" + 0.033*\"email_john\" + 0.033*\"legere_mobile\" + 0.028*\"free\"')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Business','Cooking','Team','Throwback','Company Image', 'Mocking Competition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_legere = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_legere['all_topics']= df_legere[['Business','Cooking','Team','Throwback', 'Company Image', 'Mocking Competition']].values.tolist()\n",
    "df_legere['max_topics'] = df_legere['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_legere['Business'] = df_legere.apply(lambda x: assign_topics(x['Business'], x['max_topics']), axis=1)\n",
    "df_legere['Cooking'] = df_legere.apply(lambda x: assign_topics(x['Cooking'], x['max_topics']), axis=1)\n",
    "df_legere['Team'] = df_legere.apply(lambda x: assign_topics(x['Team'], x['max_topics']), axis=1)\n",
    "df_legere['Throwback'] = df_legere.apply(lambda x: assign_topics(x['Throwback'], x['max_topics']), axis=1)\n",
    "df_legere['Company image'] = df_legere.apply(lambda x: assign_topics(x['Company Image'], x['max_topics']), axis=1)\n",
    "df_legere['Mocking Competition'] = df_legere.apply(lambda x: assign_topics(x['Mocking Competition'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_legere[['Business','Cooking','Team','Throwback', 'Company Image', 'Mocking Competition']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d7a4518>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics.index, values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for John Legere')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sundar Pichai - Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.4519016178973097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.206*\"youtube\" + 0.182*\"go\" + 0.158*\"android\" + 0.158*\"story\" + 0.134*\"susanwojcicki\" + 0.080*\"today\" + 0.014*\"learning\" + 0.004*\"like\" + 0.003*\"new\" + 0.002*\"proud\"'),\n",
       " (1,\n",
       "  '0.204*\"excited\" + 0.128*\"working\" + 0.111*\"forward\" + 0.094*\"search\" + 0.092*\"day\" + 0.081*\"first\" + 0.080*\"today\" + 0.079*\"look\" + 0.027*\"time\" + 0.019*\"thanks\"'),\n",
       " (2,\n",
       "  '0.163*\"year\" + 0.115*\"today\" + 0.104*\"help\" + 0.082*\"machine\" + 0.082*\"learning\" + 0.076*\"world\" + 0.063*\"effort\" + 0.063*\"million\" + 0.056*\"first\" + 0.052*\"people\"'),\n",
       " (3,\n",
       "  '0.628*\"google\" + 0.051*\"making\" + 0.047*\"help\" + 0.045*\"thank\" + 0.045*\"today\" + 0.040*\"u\" + 0.038*\"new\" + 0.034*\"happy\" + 0.025*\"ai\" + 0.007*\"great\"'),\n",
       " (4,\n",
       "  '0.208*\"great\" + 0.144*\"team\" + 0.135*\"live\" + 0.131*\"u\" + 0.083*\"today\" + 0.079*\"amazing\" + 0.067*\"look\" + 0.063*\"like\" + 0.026*\"time\" + 0.018*\"congrats\"'),\n",
       " (5,\n",
       "  '0.202*\"thanks\" + 0.187*\"support\" + 0.100*\"thank\" + 0.096*\"everyone\" + 0.090*\"proud\" + 0.073*\"happy\" + 0.068*\"one\" + 0.055*\"w\" + 0.040*\"time\" + 0.026*\"day\"'),\n",
       " (6,\n",
       "  '0.198*\"new\" + 0.174*\"see\" + 0.106*\"work\" + 0.096*\"ai\" + 0.077*\"googleai\" + 0.070*\"great\" + 0.062*\"digital\" + 0.057*\"congrats\" + 0.048*\"proud\" + 0.026*\"today\"')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@sundarpichai', 7)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['YouTube','New Ventures','Machine Learning','Social','Employee Appreciation','Gratitude','Products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sundar_pichai = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_sundar_pichai['all_topics']= df_sundar_pichai[['YouTube','New Ventures','Machine Learning','Social','Employee Appreciation','Gratitude','Products']].values.tolist()\n",
    "df_sundar_pichai['max_topics'] = df_sundar_pichai['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_sundar_pichai['YouTube'] = df_sundar_pichai.apply(lambda x: assign_topics(x['YouTube'], x['max_topics']), axis=1)\n",
    "df_sundar_pichai['New Ventures'] = df_sundar_pichai.apply(lambda x: assign_topics(x['New Ventures'], x['max_topics']), axis=1)\n",
    "df_sundar_pichai['Machine Learning'] = df_sundar_pichai.apply(lambda x: assign_topics(x['Machine Learning'], x['max_topics']), axis=1)\n",
    "df_sundar_pichai['Social'] = df_sundar_pichai.apply(lambda x: assign_topics(x['Social'], x['max_topics']), axis=1)\n",
    "df_sundar_pichai['Employee Appreciation'] = df_sundar_pichai.apply(lambda x: assign_topics(x['Employee Appreciation'], x['max_topics']), axis=1)\n",
    "df_sundar_pichai['Gratitude'] = df_sundar_pichai.apply(lambda x: assign_topics(x['Gratitude'], x['max_topics']), axis=1)\n",
    "df_sundar_pichai['Products'] = df_sundar_pichai.apply(lambda x: assign_topics(x['Products'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_sundar_pichai[['YouTube','New Ventures','Machine Learning','Social','Employee Appreciation','Gratitude','Products']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)\n",
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics['index'] = ceo_topics['index'].apply(lambda x:\"Products and Services\" if any(y in x.lower() for y in [\"new ventures\",\"products\"]) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d716eb8>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Sundar Pichai')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aaron Levie - Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.4643821278187853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.170*\"new\" + 0.099*\"get\" + 0.078*\"work\" + 0.072*\"day\" + 0.072*\"know\" + 0.060*\"amazon\" + 0.060*\"people\" + 0.053*\"go\" + 0.052*\"congrats\" + 0.049*\"world\"'),\n",
       " (1,\n",
       "  '0.091*\"boxworks\" + 0.084*\"time\" + 0.080*\"need\" + 0.069*\"america\" + 0.062*\"best\" + 0.054*\"future\" + 0.052*\"work\" + 0.049*\"box\" + 0.044*\"ever\" + 0.044*\"business\"'),\n",
       " (2,\n",
       "  '0.246*\"box\" + 0.116*\"boxhq\" + 0.101*\"year\" + 0.053*\"excited\" + 0.051*\"today\" + 0.051*\"u\" + 0.038*\"great\" + 0.028*\"working\" + 0.028*\"moment\" + 0.028*\"going\"'),\n",
       " (3,\n",
       "  '0.104*\"one\" + 0.064*\"software\" + 0.059*\"startup\" + 0.055*\"like\" + 0.053*\"enterprise\" + 0.050*\"company\" + 0.050*\"1\" + 0.044*\"2\" + 0.041*\"every\" + 0.038*\"never\"')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@levie', 4)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Supporting Competitors','Culture','Company','Innovation/Launch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_levie = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_levie['all_topics']= df_levie[['Supporting Competitors','Culture','Company','Innovation/Launch']].values.tolist()\n",
    "df_levie['max_topics'] = df_levie['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_levie['Supporting Competitors'] = df_levie.apply(lambda x: assign_topics(x['Supporting Competitors'], x['max_topics']), axis=1)\n",
    "df_levie['Culture'] = df_levie.apply(lambda x: assign_topics(x['Culture'], x['max_topics']), axis=1)\n",
    "df_levie['Company'] = df_levie.apply(lambda x: assign_topics(x['Company'], x['max_topics']), axis=1)\n",
    "df_levie['Innovation/Launch'] = df_levie.apply(lambda x: assign_topics(x['Innovation/Launch'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_levie[['Supporting Competitors','Culture','Company','Innovation/Launch']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)\n",
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d721898>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Aaron Levie')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Michael Dell - Dell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.3929482944080737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.134*\"delltech\" + 0.128*\"vmware\" + 0.115*\"pivotal\" + 0.075*\"cloud\" + 0.074*\"dellemc\" + 0.050*\"pivotalcf\" + 0.040*\"vmwarensx\" + 0.037*\"virtustream\" + 0.025*\"together\" + 0.022*\"secureworks\"'),\n",
       " (1,\n",
       "  '0.135*\"thanks\" + 0.082*\"1\" + 0.052*\"year\" + 0.044*\"customer\" + 0.044*\"delltech\" + 0.040*\"dellemc\" + 0.039*\"msdf_foundation\" + 0.038*\"customer_partner\" + 0.036*\"making\" + 0.033*\"texas\"'),\n",
       " (2,\n",
       "  '0.093*\"data\" + 0.072*\"delltech\" + 0.050*\"digital\" + 0.049*\"world\" + 0.045*\"help\" + 0.040*\"join\" + 0.038*\"transformation\" + 0.034*\"share\" + 0.033*\"company\" + 0.033*\"make\"'),\n",
       " (3,\n",
       "  '0.150*\"dell\" + 0.096*\"great\" + 0.083*\"delltech\" + 0.060*\"see\" + 0.038*\"dellemc\" + 0.031*\"technology\" + 0.026*\"always\" + 0.023*\"rsasecurity\" + 0.021*\"proud\" + 0.017*\"part\"'),\n",
       " (4,\n",
       "  '0.101*\"vmware\" + 0.078*\"pgelsinger\" + 0.057*\"congratulation\" + 0.040*\"team\" + 0.038*\"vmworld\" + 0.038*\"spoonen\" + 0.035*\"agree\" + 0.033*\"great\" + 0.033*\"delltechworld\" + 0.025*\"looking_forward\"'),\n",
       " (5,\n",
       "  '0.090*\"team\" + 0.089*\"delltech\" + 0.059*\"new\" + 0.049*\"great\" + 0.043*\"work\" + 0.039*\"proud\" + 0.038*\"support\" + 0.034*\"awesome\" + 0.034*\"thanks\" + 0.032*\"u\"')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@MichaelDell', 6)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['VMVare','Gratitude','Motto','RSA','Team','Pride']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_dell['all_topics']= df_dell[['VMVare','Gratitude','Motto','RSA','Team','Pride']].values.tolist()\n",
    "df_dell['max_topics'] = df_dell['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_dell['VMVare'] = df_dell.apply(lambda x: assign_topics(x['VMVare'], x['max_topics']), axis=1)\n",
    "df_dell['Gratitude'] = df_dell.apply(lambda x: assign_topics(x['Gratitude'], x['max_topics']), axis=1)\n",
    "df_dell['Motto'] = df_dell.apply(lambda x: assign_topics(x['Motto'], x['max_topics']), axis=1)\n",
    "df_dell['RSA'] = df_dell.apply(lambda x: assign_topics(x['RSA'], x['max_topics']), axis=1)\n",
    "df_dell['Team'] = df_dell.apply(lambda x: assign_topics(x['Team'], x['max_topics']), axis=1)\n",
    "df_dell['Pride'] = df_dell.apply(lambda x: assign_topics(x['Pride'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_dell[['VMVare','Gratitude','Motto','RSA','Team','Pride']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)\n",
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d802080>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Michael Dell')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eldsjal (Spotify CEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.5502476442199072\n"
     ]
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@eldsjal', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.199*\"spotify\" + 0.154*\"sure\" + 0.153*\"team\" + 0.123*\"padmasree\" + 0.123*\"excited\" + 0.113*\"back\" + 0.043*\"time\" + 0.023*\"thank\" + 0.009*\"spotifyartists\" + 0.003*\"love\"'),\n",
       " (1,\n",
       "  '0.453*\"spotify\" + 0.081*\"mpawlo\" + 0.074*\"great\" + 0.060*\"time\" + 0.046*\"du\" + 0.046*\"eldsjal\" + 0.046*\"thank\" + 0.039*\"playlist\" + 0.036*\"say\" + 0.035*\"podcast\"'),\n",
       " (2,\n",
       "  '0.118*\"new\" + 0.117*\"today\" + 0.110*\"good\" + 0.110*\"year\" + 0.090*\"like\" + 0.090*\"yes\" + 0.090*\"u\" + 0.083*\"know\" + 0.073*\"music\" + 0.050*\"spotifyartists\"'),\n",
       " (3,\n",
       "  '0.178*\"one\" + 0.166*\"love\" + 0.143*\"mootron\" + 0.143*\"get\" + 0.119*\"fulhack\" + 0.119*\"vulfpeck\" + 0.057*\"take\" + 0.004*\"team\" + 0.003*\"spotify\" + 0.003*\"new\"')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Award','Colleague','Future','Artist']\n",
    "\n",
    "df_spotify = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_spotify['all_topics']= df_spotify[['Award','Colleague','Future','Artist']].values.tolist()\n",
    "df_spotify['max_topics'] = df_spotify['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_spotify['Award'] = df_spotify.apply(lambda x: assign_topics(x['Award'], x['max_topics']), axis=1)\n",
    "df_spotify['Colleague'] = df_spotify.apply(lambda x: assign_topics(x['Colleague'], x['max_topics']), axis=1)\n",
    "df_spotify['Future'] = df_spotify.apply(lambda x: assign_topics(x['Future'], x['max_topics']), axis=1)\n",
    "df_spotify['Artist'] = df_spotify.apply(lambda x: assign_topics(x['Artist'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_spotify[['Award','Colleague','Future','Artist']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d6ef550>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Spotify')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anand Mahindra - Mahindra Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.37626067254598317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.043*\"way\" + 0.037*\"go\" + 0.035*\"thank\" + 0.028*\"pic\" + 0.024*\"week\" + 0.024*\"take\" + 0.022*\"2\" + 0.021*\"1\" + 0.021*\"tech_mahindra\" + 0.019*\"mahindraracing\"'),\n",
       " (1,\n",
       "  '0.064*\"epicchannelin\" + 0.063*\"day\" + 0.046*\"well\" + 0.043*\"never\" + 0.040*\"happy\" + 0.035*\"team\" + 0.029*\"give\" + 0.028*\"work\" + 0.026*\"support_passioneering\" + 0.025*\"fanboost_vote\"'),\n",
       " (2,\n",
       "  '0.042*\"get\" + 0.035*\"one\" + 0.032*\"great\" + 0.029*\"even\" + 0.027*\"offthebeatentrack\" + 0.024*\"country\" + 0.019*\"much\" + 0.019*\"always\" + 0.018*\"list_entry\" + 0.018*\"today_bucket\"'),\n",
       " (3,\n",
       "  '0.028*\"mahindra\" + 0.027*\"design\" + 0.027*\"name\" + 0.022*\"new\" + 0.020*\"anandmahindra\" + 0.020*\"car\" + 0.020*\"big\" + 0.020*\"award\" + 0.019*\"one\" + 0.018*\"pininfarina\"'),\n",
       " (4,\n",
       "  '0.038*\"year\" + 0.031*\"right\" + 0.024*\"true\" + 0.022*\"mahindraracing\" + 0.020*\"jawa\" + 0.020*\"anandmahindra\" + 0.019*\"many\" + 0.019*\"mumbai\" + 0.019*\"looking\" + 0.018*\"jawamotorcycles\"'),\n",
       " (5,\n",
       "  '0.063*\"today\" + 0.036*\"yes\" + 0.034*\"keep\" + 0.032*\"good\" + 0.032*\"made\" + 0.028*\"day\" + 0.028*\"may\" + 0.027*\"news\" + 0.024*\"sunday\" + 0.019*\"story\"'),\n",
       " (6,\n",
       "  '0.050*\"world\" + 0.041*\"like\" + 0.036*\"life\" + 0.032*\"think\" + 0.032*\"anandmahindra\" + 0.031*\"business\" + 0.028*\"would\" + 0.025*\"need\" + 0.023*\"thing\" + 0.020*\"people\"'),\n",
       " (7,\n",
       "  '0.090*\"mahindra\" + 0.041*\"look\" + 0.032*\"let\" + 0.032*\"new\" + 0.026*\"first\" + 0.025*\"india\" + 0.022*\"watch\" + 0.020*\"hope\" + 0.019*\"word\" + 0.018*\"anandmahindra\"'),\n",
       " (8,\n",
       "  '0.057*\"u\" + 0.057*\"make\" + 0.044*\"mahindrarise\" + 0.033*\"want\" + 0.031*\"proud\" + 0.026*\"company\" + 0.019*\"e\" + 0.019*\"woman\" + 0.018*\"sure\" + 0.018*\"indian\"'),\n",
       " (9,\n",
       "  '0.052*\"anandmahindra\" + 0.030*\"क\" + 0.025*\"post\" + 0.024*\"one\" + 0.023*\"ह\" + 0.023*\"match\" + 0.021*\"whatsappwonderbox\" + 0.020*\"त\" + 0.019*\"र\" + 0.019*\"saw\"')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@anandmahindra', 10)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Race','Formula1','Travel/Tourism','Launch/Innovation', 'Product',\n",
    "                      'Personal Story', 'Business', 'Appreciation', 'Electric Car', 'Entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mahindra = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_mahindra['all_topics']= df_mahindra[['Race','Formula1','Travel/Tourism','Launch/Innovation', 'Product',\n",
    "                      'Personal Story', 'Business', 'Appreciation', 'Electric Car', 'Entertainment']].values.tolist()\n",
    "df_mahindra['max_topics'] = df_mahindra['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_mahindra['Race'] = df_mahindra.apply(lambda x: assign_topics(x['Race'], x['max_topics']), axis=1)\n",
    "df_mahindra['Formula1'] = df_mahindra.apply(lambda x: assign_topics(x['Formula1'], x['max_topics']), axis=1)\n",
    "df_mahindra['Travel/Tourism'] = df_mahindra.apply(lambda x: assign_topics(x['Travel/Tourism'], x['max_topics']), axis=1)\n",
    "df_mahindra['Launch/Innovation'] = df_mahindra.apply(lambda x: assign_topics(x['Launch/Innovation'], x['max_topics']), axis=1)\n",
    "df_mahindra['Product'] = df_mahindra.apply(lambda x: assign_topics(x['Product'], x['max_topics']), axis=1)\n",
    "df_mahindra['Personal Story'] = df_mahindra.apply(lambda x: assign_topics(x['Personal Story'], x['max_topics']), axis=1)\n",
    "df_mahindra['Business'] = df_mahindra.apply(lambda x: assign_topics(x['Business'], x['max_topics']), axis=1)\n",
    "df_mahindra['Appreciation'] = df_mahindra.apply(lambda x: assign_topics(x['Appreciation'], x['max_topics']), axis=1)\n",
    "df_mahindra['Electric Car'] = df_mahindra.apply(lambda x: assign_topics(x['Electric Car'], x['max_topics']), axis=1)\n",
    "df_mahindra['Entertainment'] = df_mahindra.apply(lambda x: assign_topics(x['Entertainment'], x['max_topics']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_mahindra[['Race','Formula1','Travel/Tourism','Launch/Innovation', 'Product',\n",
    "                      'Personal Story', 'Business', 'Appreciation', 'Electric Car', 'Entertainment']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d87c400>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Spotify')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mary Barra - GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence of the LDA model is 0.43600020608260975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.563*\"gm\" + 0.107*\"future\" + 0.083*\"proud\" + 0.077*\"vision\" + 0.072*\"work\" + 0.060*\"congratulation\" + 0.003*\"world\" + 0.003*\"today\" + 0.003*\"thanks\" + 0.003*\"stem\"'),\n",
       " (1,\n",
       "  '0.174*\"year\" + 0.153*\"car\" + 0.152*\"thank\" + 0.131*\"business\" + 0.121*\"driving\" + 0.110*\"self\" + 0.064*\"great\" + 0.050*\"congratulation\" + 0.004*\"woman\" + 0.004*\"time\"'),\n",
       " (2,\n",
       "  '0.175*\"today\" + 0.141*\"great\" + 0.124*\"world\" + 0.117*\"team\" + 0.103*\"stem\" + 0.096*\"thanks\" + 0.088*\"woman\" + 0.081*\"time\" + 0.040*\"proud\" + 0.007*\"work\"')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceo, lda_results, train_vecs, num_topics = lda_analysis(df, '@mtbarra', 3)\n",
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df=pd.DataFrame(train_vecs)\n",
    "train_vec_df.columns=['Company Vision','Products','Social/Appreciation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mtbarra = pd.concat([df_ceo.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
    "df_mtbarra['all_topics']= df_mtbarra[['Company Vision','Products','Social/Appreciation']].values.tolist()\n",
    "df_mtbarra['max_topics'] = df_mtbarra['all_topics'].apply(lambda values: get_max_topics(values))\n",
    "\n",
    "df_mtbarra['Company Vision'] = df_mtbarra.apply(lambda x: assign_topics(x['Company Vision'], x['max_topics']), axis=1)\n",
    "df_mtbarra['Products'] = df_mtbarra.apply(lambda x: assign_topics(x['Products'], x['max_topics']), axis=1)\n",
    "df_mtbarra['Social/Appreciation'] = df_mtbarra.apply(lambda x: assign_topics(x['Social/Appreciation'], x['max_topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topic_weights = df_mtbarra[['Company Vision','Products','Social/Appreciation']].sum(axis=0)\n",
    "ceo_topics = pd.DataFrame(average_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_topics = ceo_topics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~sah_lumos/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2408d875630>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from palettable.colorbrewer.diverging import *\n",
    "\n",
    "from palettable.cmocean.sequential import Ice_10\n",
    "colors = Ice_10.hex_colors\n",
    "\n",
    "\n",
    "topics_pie = go.Pie(labels=ceo_topics[\"index\"], values=ceo_topics[0], marker=dict(colors=colors\n",
    "                                                            , line=dict(color='#FFF', width=2)),\n",
    "                                                            domain={'x': [0.0, .4], 'y': [0.0, 1]}\n",
    "                                                            , showlegend=False, textinfo='label+percent')\n",
    "\n",
    "layout = go.Layout(height = 600,\n",
    "                   width = 1000,\n",
    "                   autosize = False,\n",
    "                   title = 'Topic Distribution for Mary Barra')\n",
    "fig = go.Figure(data = topics_pie, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic_pie_chart')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
